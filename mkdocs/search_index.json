{
    "docs": [
        {
            "location": "/", 
            "text": "Histogram-weighted Networks (hiwenet)\n\n\n\n\n\n\n\n\n\n\nHistogram-weighted Networks for Feature Extraction and Advanced Analysis in Neuroscience\n\n\nNetwork-level analysis of various features, esp. if it can be individualized for a single-subject, is proving to be quite a valuable tool in many applications. This package extracts single-subject (individualized, or intrinsic) networks from node-wise data by computing the edge weights based on histogram distance between the distributions of values within each node. Individual nodes could be an ROI or a patch or a cube, or any other unit of relevance in your application. This is a great way to take advantage of the full distribution of values available within each node, relative to the simpler use of averages (or another summary statistic). \n\n\nRough scheme of computation is shown below:\n\n\n\nNote on applicability\n \n\n\nAlthough this technique was originally developed for cortical thickness, this is a generic and powerful technique that could be applied to any features such as gray matter density, PET uptake values, functional activation data or EEG features. All you need is a set of nodes/parcellation that has one-to-one correspondence across samples/subjects in your dataset.\n\n\nReferences\n\n\nA publication outlining one use case is here:\n\nRaamana, P.R. and Strother, S.C., 2016, June. Novel histogram-weighted cortical thickness networks and a multi-scale analysis of predictive power in Alzheimer's disease. In Pattern Recognition in Neuroimaging (PRNI), 2016 International Workshop on (pp. 1-4). IEEE.\n\n\nAnother poster describing it can be found here: https://doi.org/10.6084/m9.figshare.5241616\n\n\nInstallation\n\n\npip install -U hiwenet\n\n\nI don't expect the core computation to change at all, but it's always a good idea to update regularly (-U is key in the above command) to ensure you get all the bug-fixes, new features and optimized performance. \n\n\nCitation\n\n\nIf you found it useful for your research, please cite it as:\n\n\n\n\nPradeep Reddy Raamana. (Version 2). Histogram-weighted Networks for Feature Extraction, Connectivity and Advanced Analysis in Neuroscience. Zenodo. http://doi.org/10.5281/zenodo.839995", 
            "title": "Home"
        }, 
        {
            "location": "/#histogram-weighted-networks-hiwenet", 
            "text": "Histogram-weighted Networks for Feature Extraction and Advanced Analysis in Neuroscience  Network-level analysis of various features, esp. if it can be individualized for a single-subject, is proving to be quite a valuable tool in many applications. This package extracts single-subject (individualized, or intrinsic) networks from node-wise data by computing the edge weights based on histogram distance between the distributions of values within each node. Individual nodes could be an ROI or a patch or a cube, or any other unit of relevance in your application. This is a great way to take advantage of the full distribution of values available within each node, relative to the simpler use of averages (or another summary statistic).   Rough scheme of computation is shown below:  Note on applicability    Although this technique was originally developed for cortical thickness, this is a generic and powerful technique that could be applied to any features such as gray matter density, PET uptake values, functional activation data or EEG features. All you need is a set of nodes/parcellation that has one-to-one correspondence across samples/subjects in your dataset.", 
            "title": "Histogram-weighted Networks (hiwenet)"
        }, 
        {
            "location": "/#references", 
            "text": "A publication outlining one use case is here: Raamana, P.R. and Strother, S.C., 2016, June. Novel histogram-weighted cortical thickness networks and a multi-scale analysis of predictive power in Alzheimer's disease. In Pattern Recognition in Neuroimaging (PRNI), 2016 International Workshop on (pp. 1-4). IEEE.  Another poster describing it can be found here: https://doi.org/10.6084/m9.figshare.5241616", 
            "title": "References"
        }, 
        {
            "location": "/#installation", 
            "text": "pip install -U hiwenet  I don't expect the core computation to change at all, but it's always a good idea to update regularly (-U is key in the above command) to ensure you get all the bug-fixes, new features and optimized performance.", 
            "title": "Installation"
        }, 
        {
            "location": "/#citation", 
            "text": "If you found it useful for your research, please cite it as:   Pradeep Reddy Raamana. (Version 2). Histogram-weighted Networks for Feature Extraction, Connectivity and Advanced Analysis in Neuroscience. Zenodo. http://doi.org/10.5281/zenodo.839995", 
            "title": "Citation"
        }, 
        {
            "location": "/api_ref/", 
            "text": "Usage\n\n\nThis package computes single-subject networks, hence you may need loop over samples/subjects in your dataset to extract them for all the samples/subjects, and them proceed to your subsequent analysis (such as classification etc).\n\n\nA rough example for API usage can be:\n\n\nfrom hiwenet import extract as hiwenet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport nibabel\nimport os\n\n# ----------------------------------------------------\n# toy examples - modify for your application/need\nmy_project = '/data/myproject'\nsubject_list = ['a1', 'b2', 'c3', 'd4']\nsubject_labels = [1, 1, -1, -1]\n\nnum_subjects = len(subject_list)\n# number of features (imaging vertex-wise cortical thickness values over the whole brain)\nfeature_dimensionality = 1000 \nnum_ROIs = 50\nedge_weights = np.empty(num_subjects, num_ROIs*(num_ROIs-1)/2.0)\n\natlas = 'fsaverage'\n# ----------------------------------------------------\n\ndef get_parcellation(atlas, parcel_param):\n    \nPlaceholder to insert your own function to return parcellation in reference space.\n\n    parc_path = os.path.join(atlas, 'parcellation_param{}.mgh'.format(parcel_param))\n    parcel = nibabel.freesurfer.io.read_geometry(parc_path)\n\n    return parcel\n\n\ngroups = get_parcellation(atlas, feature_dimensionality)\n\nout_folder = os.path.join(my_project, 'hiwenet')\n\n# choose a method from one among the three groups (metrics, semi-metrics and similarity functions)\nmetrics = [ 'manhattan', 'minowski', 'euclidean', 'noelle_2', 'noelle_4', 'noelle_5' ]\n\nsemi_metric_list = [\n    'kullback_leibler', 'cosine_1', \n    'jensen_shannon', 'chi_square',\n    'chebyshev', 'chebyshev_neg',\n    'histogram_intersection_1',\n    'relative_deviation', 'relative_bin_deviation',\n    'noelle_1', 'noelle_3',\n    'correlate_1']\nsimilarity_func = ['correlate', 'cosine', 'cosine_2', 'cosine_alt', 'fidelity_based']\n\n\n\ndef get_features(subject_id):\n    \nPlaceholder to insert your own function to read subject-wise features.\n\n    features_path = os.path.join(my_project,'base_features', subject_id, 'features.txt')\n    feature_vector = np.loadtxt(features_path)\n\n    return feature_vector\n\n\ndef upper_tri_vec(matrix):\n    \nReturns the vectorized values of upper triangular part of a matrix\n\n\n    triu_idx = np.triu_indices_from(matrix, 1)\n    return matrix[triu_idx]\n\n\nfor ss, subject in enumerate(subject_list):\n  features = get_features(subject)\n  edge_weights_subject = hiwenet(features, groups) # or add weight_method = metrics[ii] to use a various other metrics \n  edge_weights[ss,:] = upper_tri_vec(edge_weights_subject)\n\n  out_file = os.path.join(out_folder, 'hiwenet_{}.txt'.format(subject))\n  np.save(out_file, edge_weights_subject)\n\n\n# proceed to analysis\n\n\n# very rough example for training/evaluating a classifier\nrf = RandomForestClassifier(oob_score = True)\nscores = cross_val_score(rf, edge_weights, subject_labels)", 
            "title": "API Reference"
        }, 
        {
            "location": "/api_ref/#usage", 
            "text": "This package computes single-subject networks, hence you may need loop over samples/subjects in your dataset to extract them for all the samples/subjects, and them proceed to your subsequent analysis (such as classification etc).  A rough example for API usage can be:  from hiwenet import extract as hiwenet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport nibabel\nimport os\n\n# ----------------------------------------------------\n# toy examples - modify for your application/need\nmy_project = '/data/myproject'\nsubject_list = ['a1', 'b2', 'c3', 'd4']\nsubject_labels = [1, 1, -1, -1]\n\nnum_subjects = len(subject_list)\n# number of features (imaging vertex-wise cortical thickness values over the whole brain)\nfeature_dimensionality = 1000 \nnum_ROIs = 50\nedge_weights = np.empty(num_subjects, num_ROIs*(num_ROIs-1)/2.0)\n\natlas = 'fsaverage'\n# ----------------------------------------------------\n\ndef get_parcellation(atlas, parcel_param):\n     Placeholder to insert your own function to return parcellation in reference space. \n    parc_path = os.path.join(atlas, 'parcellation_param{}.mgh'.format(parcel_param))\n    parcel = nibabel.freesurfer.io.read_geometry(parc_path)\n\n    return parcel\n\n\ngroups = get_parcellation(atlas, feature_dimensionality)\n\nout_folder = os.path.join(my_project, 'hiwenet')\n\n# choose a method from one among the three groups (metrics, semi-metrics and similarity functions)\nmetrics = [ 'manhattan', 'minowski', 'euclidean', 'noelle_2', 'noelle_4', 'noelle_5' ]\n\nsemi_metric_list = [\n    'kullback_leibler', 'cosine_1', \n    'jensen_shannon', 'chi_square',\n    'chebyshev', 'chebyshev_neg',\n    'histogram_intersection_1',\n    'relative_deviation', 'relative_bin_deviation',\n    'noelle_1', 'noelle_3',\n    'correlate_1']\nsimilarity_func = ['correlate', 'cosine', 'cosine_2', 'cosine_alt', 'fidelity_based']\n\n\n\ndef get_features(subject_id):\n     Placeholder to insert your own function to read subject-wise features. \n    features_path = os.path.join(my_project,'base_features', subject_id, 'features.txt')\n    feature_vector = np.loadtxt(features_path)\n\n    return feature_vector\n\n\ndef upper_tri_vec(matrix):\n     Returns the vectorized values of upper triangular part of a matrix \n\n    triu_idx = np.triu_indices_from(matrix, 1)\n    return matrix[triu_idx]\n\n\nfor ss, subject in enumerate(subject_list):\n  features = get_features(subject)\n  edge_weights_subject = hiwenet(features, groups) # or add weight_method = metrics[ii] to use a various other metrics \n  edge_weights[ss,:] = upper_tri_vec(edge_weights_subject)\n\n  out_file = os.path.join(out_folder, 'hiwenet_{}.txt'.format(subject))\n  np.save(out_file, edge_weights_subject)\n\n\n# proceed to analysis\n\n\n# very rough example for training/evaluating a classifier\nrf = RandomForestClassifier(oob_score = True)\nscores = cross_val_score(rf, edge_weights, subject_labels)", 
            "title": "Usage"
        }, 
        {
            "location": "/api_usage/", 
            "text": "Usage\n\n\nThis package computes single-subject networks, hence you may need loop over samples/subjects in your dataset to extract them for all the samples/subjects, and them proceed to your subsequent analysis (such as classification etc).\n\n\nA rough example for API usage can be:\n\n\nfrom hiwenet import extract as hiwenet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport nibabel\nimport os\n\n# ----------------------------------------------------\n# toy examples - modify for your application/need\nmy_project = '/data/myproject'\nsubject_list = ['a1', 'b2', 'c3', 'd4']\nsubject_labels = [1, 1, -1, -1]\n\nnum_subjects = len(subject_list)\n# number of features (imaging vertex-wise cortical thickness values over the whole brain)\nfeature_dimensionality = 1000 \nnum_ROIs = 50\nedge_weights = np.empty(num_subjects, num_ROIs*(num_ROIs-1)/2.0)\n\natlas = 'fsaverage'\n# ----------------------------------------------------\n\ndef get_parcellation(atlas, parcel_param):\n    \nPlaceholder to insert your own function to return parcellation in reference space.\n\n    parc_path = os.path.join(atlas, 'parcellation_param{}.mgh'.format(parcel_param))\n    parcel = nibabel.freesurfer.io.read_geometry(parc_path)\n\n    return parcel\n\n\ngroups = get_parcellation(atlas, feature_dimensionality)\n\nout_folder = os.path.join(my_project, 'hiwenet')\n\n# choose a method from one among the three groups (metrics, semi-metrics and similarity functions)\nmetrics = [ 'manhattan', 'minowski', 'euclidean', 'noelle_2', 'noelle_4', 'noelle_5' ]\n\nsemi_metric_list = [\n    'kullback_leibler', 'cosine_1', \n    'jensen_shannon', 'chi_square',\n    'chebyshev', 'chebyshev_neg',\n    'histogram_intersection_1',\n    'relative_deviation', 'relative_bin_deviation',\n    'noelle_1', 'noelle_3',\n    'correlate_1']\nsimilarity_func = ['correlate', 'cosine', 'cosine_2', 'cosine_alt', 'fidelity_based']\n\n\n\ndef get_features(subject_id):\n    \nPlaceholder to insert your own function to read subject-wise features.\n\n    features_path = os.path.join(my_project,'base_features', subject_id, 'features.txt')\n    feature_vector = np.loadtxt(features_path)\n\n    return feature_vector\n\n\ndef upper_tri_vec(matrix):\n    \nReturns the vectorized values of upper triangular part of a matrix\n\n\n    triu_idx = np.triu_indices_from(matrix, 1)\n    return matrix[triu_idx]\n\n\nfor ss, subject in enumerate(subject_list):\n  features = get_features(subject)\n  edge_weights_subject = hiwenet(features, groups) # or add weight_method = metrics[ii] to use a various other metrics \n  edge_weights[ss,:] = upper_tri_vec(edge_weights_subject)\n\n  out_file = os.path.join(out_folder, 'hiwenet_{}.txt'.format(subject))\n  np.save(out_file, edge_weights_subject)\n\n\n# proceed to analysis\n\n\n# very rough example for training/evaluating a classifier\nrf = RandomForestClassifier(oob_score = True)\nscores = cross_val_score(rf, edge_weights, subject_labels)", 
            "title": "Api usage"
        }, 
        {
            "location": "/api_usage/#usage", 
            "text": "This package computes single-subject networks, hence you may need loop over samples/subjects in your dataset to extract them for all the samples/subjects, and them proceed to your subsequent analysis (such as classification etc).  A rough example for API usage can be:  from hiwenet import extract as hiwenet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport nibabel\nimport os\n\n# ----------------------------------------------------\n# toy examples - modify for your application/need\nmy_project = '/data/myproject'\nsubject_list = ['a1', 'b2', 'c3', 'd4']\nsubject_labels = [1, 1, -1, -1]\n\nnum_subjects = len(subject_list)\n# number of features (imaging vertex-wise cortical thickness values over the whole brain)\nfeature_dimensionality = 1000 \nnum_ROIs = 50\nedge_weights = np.empty(num_subjects, num_ROIs*(num_ROIs-1)/2.0)\n\natlas = 'fsaverage'\n# ----------------------------------------------------\n\ndef get_parcellation(atlas, parcel_param):\n     Placeholder to insert your own function to return parcellation in reference space. \n    parc_path = os.path.join(atlas, 'parcellation_param{}.mgh'.format(parcel_param))\n    parcel = nibabel.freesurfer.io.read_geometry(parc_path)\n\n    return parcel\n\n\ngroups = get_parcellation(atlas, feature_dimensionality)\n\nout_folder = os.path.join(my_project, 'hiwenet')\n\n# choose a method from one among the three groups (metrics, semi-metrics and similarity functions)\nmetrics = [ 'manhattan', 'minowski', 'euclidean', 'noelle_2', 'noelle_4', 'noelle_5' ]\n\nsemi_metric_list = [\n    'kullback_leibler', 'cosine_1', \n    'jensen_shannon', 'chi_square',\n    'chebyshev', 'chebyshev_neg',\n    'histogram_intersection_1',\n    'relative_deviation', 'relative_bin_deviation',\n    'noelle_1', 'noelle_3',\n    'correlate_1']\nsimilarity_func = ['correlate', 'cosine', 'cosine_2', 'cosine_alt', 'fidelity_based']\n\n\n\ndef get_features(subject_id):\n     Placeholder to insert your own function to read subject-wise features. \n    features_path = os.path.join(my_project,'base_features', subject_id, 'features.txt')\n    feature_vector = np.loadtxt(features_path)\n\n    return feature_vector\n\n\ndef upper_tri_vec(matrix):\n     Returns the vectorized values of upper triangular part of a matrix \n\n    triu_idx = np.triu_indices_from(matrix, 1)\n    return matrix[triu_idx]\n\n\nfor ss, subject in enumerate(subject_list):\n  features = get_features(subject)\n  edge_weights_subject = hiwenet(features, groups) # or add weight_method = metrics[ii] to use a various other metrics \n  edge_weights[ss,:] = upper_tri_vec(edge_weights_subject)\n\n  out_file = os.path.join(out_folder, 'hiwenet_{}.txt'.format(subject))\n  np.save(out_file, edge_weights_subject)\n\n\n# proceed to analysis\n\n\n# very rough example for training/evaluating a classifier\nrf = RandomForestClassifier(oob_score = True)\nscores = cross_val_score(rf, edge_weights, subject_labels)", 
            "title": "Usage"
        }, 
        {
            "location": "/../examples/CLI/", 
            "text": "Histogram-weighted Networks (hiwenet)\n\n\n\n\n\n\n\n\n\n\nHistogram-weighted Networks for Feature Extraction and Advanced Analysis in Neuroscience\n\n\nNetwork-level analysis of various features, esp. if it can be individualized for a single-subject, is proving to be quite a valuable tool in many applications. This package extracts single-subject (individualized, or intrinsic) networks from node-wise data by computing the edge weights based on histogram distance between the distributions of values within each node. Individual nodes could be an ROI or a patch or a cube, or any other unit of relevance in your application. This is a great way to take advantage of the full distribution of values available within each node, relative to the simpler use of averages (or another summary statistic). \n\n\nRough scheme of computation is shown below:\n\n\n\nNote on applicability\n \n\n\nAlthough this technique was originally developed for cortical thickness, this is a generic and powerful technique that could be applied to any features such as gray matter density, PET uptake values, functional activation data or EEG features. All you need is a set of nodes/parcellation that has one-to-one correspondence across samples/subjects in your dataset.\n\n\nReferences\n\n\nA publication outlining one use case is here:\n\nRaamana, P.R. and Strother, S.C., 2016, June. Novel histogram-weighted cortical thickness networks and a multi-scale analysis of predictive power in Alzheimer's disease. In Pattern Recognition in Neuroimaging (PRNI), 2016 International Workshop on (pp. 1-4). IEEE.\n\n\nAnother poster describing it can be found here: https://doi.org/10.6084/m9.figshare.5241616\n\n\nInstallation\n\n\npip install -U hiwenet\n\n\nI don't expect the core compuation to change at all, but it's always a good idea to update regularly (-U is key in the above command) to ensure you get all the bug-fixes, new features and optimized performance. \n\n\nUsage\n\n\nThis package computes single-subject networks, hence you may need loop over samples/subjects in your dataset to extract them for all the samples/subjects, and them proceed to your subsequent analysis (such as classification etc).\n\n\nA rough example of usage can be:\n\n\nfrom hiwenet import extract as hiwenet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport nibabel\nimport os\n\n# ----------------------------------------------------\n# toy examples - modify for your application/need\nmy_project = '/data/myproject'\nsubject_list = ['a1', 'b2', 'c3', 'd4']\nsubject_labels = [1, 1, -1, -1]\n\nnum_subjects = len(subject_list)\n# number of features (imaging vertex-wise cortical thickness values over the whole brain)\nfeature_dimensionality = 1000 \nnum_ROIs = 50\nedge_weights = np.empty(num_subjects, num_ROIs*(num_ROIs-1)/2.0)\n\natlas = 'fsaverage'\n# ----------------------------------------------------\n\ndef get_parcellation(atlas, parcel_param):\n    \nPlaceholder to insert your own function to return parcellation in reference space.\n\n    parc_path = os.path.join(atlas, 'parcellation_param{}.mgh'.format(parcel_param))\n    parcel = nibabel.freesurfer.io.read_geometry(parc_path)\n\n    return parcel\n\n\ngroups = get_parcellation(atlas, feature_dimensionality)\n\nout_folder = os.path.join(my_project, 'hiwenet')\n\n# choose a method from one among the three groups (metrics, semi-metrics and similarity functions)\nmetrics = [ 'manhattan', 'minowski', 'euclidean', 'noelle_2', 'noelle_4', 'noelle_5' ]\n\nsemi_metric_list = [\n    'kullback_leibler', 'cosine_1', \n    'jensen_shannon', 'chi_square',\n    'chebyshev', 'chebyshev_neg',\n    'histogram_intersection_1',\n    'relative_deviation', 'relative_bin_deviation',\n    'noelle_1', 'noelle_3',\n    'correlate_1']\nsimilarity_func = ['correlate', 'cosine', 'cosine_2', 'cosine_alt', 'fidelity_based']\n\n\n\ndef get_features(subject_id):\n    \nPlaceholder to insert your own function to read subject-wise features.\n\n    features_path = os.path.join(my_project,'base_features', subject_id, 'features.txt')\n    feature_vector = np.loadtxt(features_path)\n\n    return feature_vector\n\n\ndef upper_tri_vec(matrix):\n    \nReturns the vectorized values of upper triangular part of a matrix\n\n\n    triu_idx = np.triu_indices_from(matrix, 1)\n    return matrix[triu_idx]\n\n\nfor ss, subject in enumerate(subject_list):\n  features = get_features(subject)\n  edge_weights_subject = hiwenet(features, groups) # or add weight_method = metrics[ii] to use a various other metrics \n  edge_weights[ss,:] = upper_tri_vec(edge_weights_subject)\n\n  out_file = os.path.join(out_folder, 'hiwenet_{}.txt'.format(subject))\n  np.save(out_file, edge_weights_subject)\n\n\n# proceed to analysis\n\n\n# very rough example for training/evaluating a classifier\nrf = RandomForestClassifier(oob_score = True)\nscores = cross_val_score(rf, edge_weights, subject_labels)\n\n\n\n\n\n\nCitation\n\n\nIf you found it useful for your research, please cite it as:\n\n\n\n\nPradeep Reddy Raamana. (Version 2). Histogram-weighted Networks for Feature Extraction, Connectivity and Advanced Analysis in Neuroscience. Zenodo. http://doi.org/10.5281/zenodo.839995", 
            "title": "Command line usage"
        }, 
        {
            "location": "/../examples/CLI/#histogram-weighted-networks-hiwenet", 
            "text": "Histogram-weighted Networks for Feature Extraction and Advanced Analysis in Neuroscience  Network-level analysis of various features, esp. if it can be individualized for a single-subject, is proving to be quite a valuable tool in many applications. This package extracts single-subject (individualized, or intrinsic) networks from node-wise data by computing the edge weights based on histogram distance between the distributions of values within each node. Individual nodes could be an ROI or a patch or a cube, or any other unit of relevance in your application. This is a great way to take advantage of the full distribution of values available within each node, relative to the simpler use of averages (or another summary statistic).   Rough scheme of computation is shown below:  Note on applicability    Although this technique was originally developed for cortical thickness, this is a generic and powerful technique that could be applied to any features such as gray matter density, PET uptake values, functional activation data or EEG features. All you need is a set of nodes/parcellation that has one-to-one correspondence across samples/subjects in your dataset.", 
            "title": "Histogram-weighted Networks (hiwenet)"
        }, 
        {
            "location": "/../examples/CLI/#references", 
            "text": "A publication outlining one use case is here: Raamana, P.R. and Strother, S.C., 2016, June. Novel histogram-weighted cortical thickness networks and a multi-scale analysis of predictive power in Alzheimer's disease. In Pattern Recognition in Neuroimaging (PRNI), 2016 International Workshop on (pp. 1-4). IEEE.  Another poster describing it can be found here: https://doi.org/10.6084/m9.figshare.5241616", 
            "title": "References"
        }, 
        {
            "location": "/../examples/CLI/#installation", 
            "text": "pip install -U hiwenet  I don't expect the core compuation to change at all, but it's always a good idea to update regularly (-U is key in the above command) to ensure you get all the bug-fixes, new features and optimized performance.", 
            "title": "Installation"
        }, 
        {
            "location": "/../examples/CLI/#usage", 
            "text": "This package computes single-subject networks, hence you may need loop over samples/subjects in your dataset to extract them for all the samples/subjects, and them proceed to your subsequent analysis (such as classification etc).  A rough example of usage can be:  from hiwenet import extract as hiwenet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport nibabel\nimport os\n\n# ----------------------------------------------------\n# toy examples - modify for your application/need\nmy_project = '/data/myproject'\nsubject_list = ['a1', 'b2', 'c3', 'd4']\nsubject_labels = [1, 1, -1, -1]\n\nnum_subjects = len(subject_list)\n# number of features (imaging vertex-wise cortical thickness values over the whole brain)\nfeature_dimensionality = 1000 \nnum_ROIs = 50\nedge_weights = np.empty(num_subjects, num_ROIs*(num_ROIs-1)/2.0)\n\natlas = 'fsaverage'\n# ----------------------------------------------------\n\ndef get_parcellation(atlas, parcel_param):\n     Placeholder to insert your own function to return parcellation in reference space. \n    parc_path = os.path.join(atlas, 'parcellation_param{}.mgh'.format(parcel_param))\n    parcel = nibabel.freesurfer.io.read_geometry(parc_path)\n\n    return parcel\n\n\ngroups = get_parcellation(atlas, feature_dimensionality)\n\nout_folder = os.path.join(my_project, 'hiwenet')\n\n# choose a method from one among the three groups (metrics, semi-metrics and similarity functions)\nmetrics = [ 'manhattan', 'minowski', 'euclidean', 'noelle_2', 'noelle_4', 'noelle_5' ]\n\nsemi_metric_list = [\n    'kullback_leibler', 'cosine_1', \n    'jensen_shannon', 'chi_square',\n    'chebyshev', 'chebyshev_neg',\n    'histogram_intersection_1',\n    'relative_deviation', 'relative_bin_deviation',\n    'noelle_1', 'noelle_3',\n    'correlate_1']\nsimilarity_func = ['correlate', 'cosine', 'cosine_2', 'cosine_alt', 'fidelity_based']\n\n\n\ndef get_features(subject_id):\n     Placeholder to insert your own function to read subject-wise features. \n    features_path = os.path.join(my_project,'base_features', subject_id, 'features.txt')\n    feature_vector = np.loadtxt(features_path)\n\n    return feature_vector\n\n\ndef upper_tri_vec(matrix):\n     Returns the vectorized values of upper triangular part of a matrix \n\n    triu_idx = np.triu_indices_from(matrix, 1)\n    return matrix[triu_idx]\n\n\nfor ss, subject in enumerate(subject_list):\n  features = get_features(subject)\n  edge_weights_subject = hiwenet(features, groups) # or add weight_method = metrics[ii] to use a various other metrics \n  edge_weights[ss,:] = upper_tri_vec(edge_weights_subject)\n\n  out_file = os.path.join(out_folder, 'hiwenet_{}.txt'.format(subject))\n  np.save(out_file, edge_weights_subject)\n\n\n# proceed to analysis\n\n\n# very rough example for training/evaluating a classifier\nrf = RandomForestClassifier(oob_score = True)\nscores = cross_val_score(rf, edge_weights, subject_labels)", 
            "title": "Usage"
        }, 
        {
            "location": "/../examples/CLI/#citation", 
            "text": "If you found it useful for your research, please cite it as:   Pradeep Reddy Raamana. (Version 2). Histogram-weighted Networks for Feature Extraction, Connectivity and Advanced Analysis in Neuroscience. Zenodo. http://doi.org/10.5281/zenodo.839995", 
            "title": "Citation"
        }, 
        {
            "location": "/cite/", 
            "text": "Citation\n\n\nIf you found it useful for your research, please cite it as:\n\n\n\n\nPradeep Reddy Raamana. (Version 2). Histogram-weighted Networks for Feature Extraction, Connectivity and Advanced Analysis in Neuroscience. Zenodo. http://doi.org/10.5281/zenodo.839995", 
            "title": "Citation"
        }, 
        {
            "location": "/cite/#citation", 
            "text": "If you found it useful for your research, please cite it as:   Pradeep Reddy Raamana. (Version 2). Histogram-weighted Networks for Feature Extraction, Connectivity and Advanced Analysis in Neuroscience. Zenodo. http://doi.org/10.5281/zenodo.839995", 
            "title": "Citation"
        }
    ]
}